# Migration Guide: AnythingLLM ‚Üí LangChain RAG

Dieser Guide erkl√§rt, wie Sie von der AnythingLLM-Integration zur neuen LangChain RAG-Integration migrieren.

## √úbersicht der √Ñnderungen

### Vorher (AnythingLLM)
- ‚úÖ Einfache Einrichtung
- ‚úÖ Workspace-basierte Dokumente
- ‚úÖ Schnelle Antworten
- ‚ùå Begrenzte Dokument-Formate (nur in AnythingLLM hochgeladen)
- ‚ùå Keine lokalen Embeddings
- ‚ùå Abh√§ngig von externem AnythingLLM-Server

### Nachher (LangChain RAG)
- ‚úÖ Lokale Dokument-Verarbeitung
- ‚úÖ Mehr Dokument-Formate (PDF, DOCX, TXT, HTML)
- ‚úÖ Semantische Suche mit Vektor-Embeddings
- ‚úÖ Vollst√§ndige Kontrolle √ºber Daten
- ‚úÖ Offline-Betrieb m√∂glich
- ‚ùå Komplexere Einrichtung erforderlich

## Migrationsschritte

### 1. Vorbereitung

#### Erforderliche Software installieren
```bash
# Python f√ºr ChromaDB
py -m venv chroma-env
chroma-env\Scripts\activate
pip install chromadb

# Ollama Modelle
ollama pull nomic-embed-text    # F√ºr Embeddings
ollama pull qwen2:7b-instruct   # F√ºr Antwort-Generierung
```

#### Services starten
```bash
# Terminal 1: ChromaDB
chroma-env\Scripts\activate
chroma run --host 0.0.0.0 --port 8000

# Terminal 2: Ollama (falls nicht l√§uft)
ollama serve
```

### 2. Konfiguration aktualisieren

#### In der App
1. **√ñffnen Sie die App** und gehen Sie zu den Einstellungen
2. **Konfigurieren Sie Ollama:**
   - Base URL: `http://localhost:11434`
3. **Konfigurieren Sie ChromaDB:**
   - Base URL: `http://localhost:8000`
4. **API Keys:** Bleiben unver√§ndert (f√ºr Detective-Agent)

### 3. Modus wechseln

#### In der Interrogation-Oberfl√§che
1. **W√§hlen Sie "LangChain RAG"** als Witness-Modus
2. **Laden Sie Ihre Dokumente hoch:**
   - Verwenden Sie den "Document Management" Tab
   - Unterst√ºtzte Formate: PDF, DOCX, TXT, HTML
   - Maximale Gr√∂√üe: 50MB pro Datei

### 4. Dokumente migrieren

#### Von AnythingLLM zu LangChain
Da die Dokumente jetzt lokal verarbeitet werden:

1. **Exportieren Sie Dokumente aus AnythingLLM** (falls n√∂tig)
2. **Laden Sie die Dokumente in die App hoch**
3. **Warten Sie auf die Verarbeitung:**
   - Dokument wird automatisch in Chunks zerlegt
   - Embeddings werden generiert
   - Speicherung in ChromaDB

#### Automatische Verarbeitung
- **Chunking:** Dokumente werden in 500-2000 Zeichen gro√üe Chunks zerlegt
- **Embeddings:** Jeder Chunk bekommt einen Vektor (nomic-embed-text)
- **Speicherung:** Vektoren werden in ChromaDB abgelegt
- **Suche:** Semantische Suche findet relevante Chunks

### 5. Erste Interrogation testen

#### Test-Setup
1. **W√§hlen Sie ein hochgeladenes Dokument**
2. **Stellen Sie eine Frage** zu dem Dokumentinhalt
3. **Beobachten Sie den Prozess:**
   - Retrieval: 5 relevante Chunks werden gefunden
   - Generation: Antwort wird basierend auf Kontext erstellt
   - Zitierung: Antworten beziehen sich auf Dokumentinhalt

#### Beispiel-Frage
```
"Welche Vitamine werden im Dokument erw√§hnt?"
```
**AnythingLLM-Antwort:** Allgemeines Wissen + Workspace-Inhalt
**LangChain RAG-Antwort:** Spezifische Informationen aus dem hochgeladenen Dokument

### 6. Feinabstimmung

#### RAG-Settings anpassen
Im "RAG Settings" Tab k√∂nnen Sie optimieren:

- **Chunk Size:** Gr√∂√üe der Text-Abschnitte (500-2000)
- **Chunk Overlap:** √úberlappung zwischen Chunks (0-500)
- **Retrieval K:** Anzahl der abgerufenen Chunks (1-20)
- **Score Threshold:** Mindest-√Ñhnlichkeit f√ºr Ergebnisse (0-1)

#### Performance-Optimierung
- **F√ºr Genauigkeit:** Kleinere Chunks, h√∂heres K
- **F√ºr Geschwindigkeit:** Gr√∂√üere Chunks, niedrigeres K
- **F√ºr Pr√§zision:** H√∂herer Score Threshold

### 7. Troubleshooting

#### H√§ufige Probleme

**ChromaDB Verbindung fehlt:**
```bash
# Pr√ºfen ob ChromaDB l√§uft
curl http://localhost:8000/api/v1/heartbeat

# Starten falls n√∂tig
chroma-env\Scripts\activate
chroma run --host 0.0.0.0 --port 8000
```

**Ollama Modelle fehlen:**
```bash
# Verf√ºgbare Modelle pr√ºfen
ollama list

# Modelle installieren
ollama pull nomic-embed-text
ollama pull qwen2:7b-instruct
```

**Dokument-Verarbeitung h√§ngt:**
- ChromaDB und Ollama m√ºssen laufen
- Dokument darf nicht besch√§digt sein
- Logs in der Konsole pr√ºfen

#### Fallback zu AnythingLLM
Falls Probleme auftreten, k√∂nnen Sie jederzeit zur√ºck zu AnythingLLM wechseln:
- W√§hlen Sie "AnythingLLM" als Witness-Modus
- Konfigurieren Sie Ihren AnythingLLM-Server
- Dokumente m√ºssen in AnythingLLM hochgeladen sein

## Vorteile der Migration

### Qualit√§t
- **Kontextbezogene Antworten:** Nur aus hochgeladenen Dokumenten
- **Keine Halluzinationen:** Antworten basieren auf tats√§chlichem Inhalt
- **Zitierf√§higkeit:** Jede Antwort l√§sst sich zu Quellen zur√ºckverfolgen

### Kontrolle
- **Lokale Verarbeitung:** Keine Abh√§ngigkeit von externen Services
- **Datenschutz:** Dokumente bleiben auf Ihrem Rechner
- **Offline-Betrieb:** Funktioniert ohne Internet (au√üer f√ºr Detective)

### Flexibilit√§t
- **Mehr Formate:** PDF, DOCX zus√§tzlich zu TXT/HTML
- **Skalierbarkeit:** Mehrere Dokumente gleichzeitig
- **Anpassbarkeit:** Fine-Tuning der RAG-Parameter

## Performance-Vergleich

| Aspekt | AnythingLLM | LangChain RAG |
|--------|-------------|----------------|
| Setup-Komplexit√§t | Einfach | Mittel |
| Dokument-Formate | Begrenzt | Vollst√§ndig |
| Antwort-Qualit√§t | Gut | Sehr gut |
| Datenschutz | Mittel | Hoch |
| Offline-Betrieb | Nein | Ja |
| Anpassbarkeit | Begrenzt | Hoch |

## Support

Bei Problemen:
1. **Logs pr√ºfen:** Konsole-Ausgaben enthalten detaillierte Fehlermeldungen
2. **Dokumentation:** README.md enth√§lt Troubleshooting-Sektion
3. **Test-Dokumente:** Verwenden Sie die bereitgestellten Test-Dateien

Die Migration ist **r√ºckw√§rtskompatibel** - Sie k√∂nnen jederzeit zwischen Modi wechseln! üîÑ